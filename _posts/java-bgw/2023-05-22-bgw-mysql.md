---
layout: post
title: Java八股文 之 MySQL
categories: [八股文]
description: Java八股文 之 MySQL
keywords: 八股文
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---

> MySQL 是一种关系型数据库，主要用于持久化存储我们的系统中的一些数据比如用户信息。

# 什么是关系型数据库

关系型数据库就是一种建立在关系模型基础上的数据库。关系模型表明了数据库中所存储的数据之间的联系（一对一、一对多、多对多）。

# 除了 MySQL 还有哪些关系型数据库呢？

Oracle、SQL Server、SQLite（微信本地的聊天记录的存储就是用的 SQLite）

# MySQL 的存储引擎

MySQL 5.5 版本之前，MyISAM 是 MySQL 的默认存储引擎

MySQL 5.5 版本开始，InnoDB 成为 MySQL 的默认存储引擎

# MyISAM 与 InnoDB 之间的区别

是否支持事务
- MyISAM 不支持事务
- InnoDB 支持事务

锁粒度
- MyISAM 只有表级锁(table-level locking)，
- InnoDB 支持行级锁(row-level locking)和表级锁，默认为行级锁，可以支持更高的并发

外键
- MyISAM 不支持
- InnoDB 支持

数据库异常崩溃后的安全恢复
- MyISAM 不支持
- InnoDB 支持

# 如何选择默认存储引擎

一般情况下都会选择 InnoDB。如果不考虑扩展能力、并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题，也可以选择 MyIsam。

# 说一下表级锁和行级锁

- 表级锁： MySQL 中锁定 **粒度最大** 的一种锁，对当前操作的整张表加锁，MyISAM 和 InnoDB 引擎都支持表级锁。优点：实现简单，资源消耗也比较少，加锁快，不会出现死锁；缺点：其锁定粒度最大，触发锁冲突的概率最高，并发度最低。
- 行级锁： MySQL 中锁定 **粒度最小** 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。

# 说一下悲观锁

悲观锁是一种并发控制的方法，它指的是对数据被外界修改持悲观态度，说通俗点，就是认为数据总是会被外界修改，因此在整个数据处理过程中，会让数据处于锁定状态，

悲观锁往往依靠数据库提供的锁机制来实现，因为只有数据库才能真正保证数据访问的排他性，要使用悲观锁，必须关闭 MySQL 的自动提交属性，也就是 set commit = 0;

**优点与不足**

悲观锁这种「先取锁再访问」的保守策略，为数据处理的安全提供了保证，但是在效率方面处理加锁的机制会让数据库产生额外的开销，还会增加产生死锁的机会，还有一个事务如果锁定了某行数据，其它事务就必须等待该事务处理完才可以处理那行数据，这会使并行性大大降低。

# 说一下乐观锁

乐观锁（ Optimistic Locking ） 也是一种并发控制的方法，相对悲观锁而言，它会假设数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会对数据的冲突与否进行检测，如果发现冲突了，则返回用户错误的信息，让用户决定如何去做。

一般会使用版本号机制实现实现乐观锁，在数据初始化时指定一个版本号，每次对数据的更新操作都对版本号执行 +1 操作。并判断当前版本号是不是该数据最新的版本号。

**优点与不足**

乐观锁相信事务之间的冲突概率是比较小的，因此会尽可能的去做，直到提交的时候才去锁定，所以不会产生任何锁和死锁。

# CHAR 和 VARCHAR 的区别？

1. CHAR 和 VARCHAR 类型在存储和检索方面有所不同
2. CHAR 列长度固定为创建表时声明的长度，长度值范围是 1 到 255 当 CHAR值被存储时，它们被用空格填充到特定长度，检索 CHAR 值时需删除尾随空格。

# 说一说 drop、delete 与 truncate 的区别

SQL 中的 drop、delete、truncate 都表示删除，但是三者有一些差别

delete 和 truncate 只删除表的数据不删除表的结构

速度来说：drop > truncate > delete

delete 语句是 dml，这个操作会放到 rollback segement 中，事务提交之后才生效，如果有相应的trigger，执行的时候将被触发.

truncate，drop是 ddl，操作立即生效，原数据不放到 rollback segment中，不能回滚， 操作不触发 trigger

# 什么是事务

简单来说，就是在执行一组整体逻辑的时候，要么全部执行成功，要么全部不执行。

# 事务的 ACID 特性

1. **原子性**（Atomicity） ： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；
2. **一致性**（Consistency）： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；
3. **隔离性**（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；
4. **持久性**（Durabilily）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。

# 数据库的事务是怎么实现的呢

以 MySQL 的 InnoDB 引擎为例：

InnoDB 引擎使用 **redo log(重做日志)** 保证事务的**持久性**，使用 undo log(回滚日志) 来保证事务的原子性。

InnoDB 引擎通过 **锁机制、MVCC** 等手段来保证事务的隔离性（ 默认支持的隔离级别是 **REPEATABLE-READ** ）。

保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。

# 并发事务会带来哪些问题

在应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。

- **脏读（Dirty read）**: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是「脏数据」，依据「脏数据」所做的操作可能是不正确的。
- **丢失修改（Lost to modify）**: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，如果第一个事务中修改了这个数据后，第二个事务也修改了这个数据。那第一个事务的修改结果就会被丢失，因此称为丢失修改。 例如：事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 修改 A=A-1，事务 2 也修改 A=A-1，最终结果 A=19，事务 1 的修改被丢失。
- **不可重复读（Unrepeatable read）**: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
- **幻读（Phantom read）**: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

# 事务隔离级别

- READ-UNCOMMITTED(读未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。
- READ-COMMITTED(读已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。
- REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生，MySQL默认。
- SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。

# 什么是索引

索引是一种用于快速查询和检索数据的数据结构，相当于一本书当中的目录。

# 索引的优缺点

优点

- 使用索引可以大大加快数据的检索速度（大大减少检索的数据量）, 这也是创建索引的最主要的原因。
- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。

缺点

- 建索引和维护索引需要耗费许多时间。当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。
- 索引需要使用物理文件存储，也会耗费一定空间。

# 索引一定能提高查询性能吗？

大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。

# 索引的底层是如何实现的？

采用的 B+ 树

# 为什么不采用 B 树作为底层结构呢？

1. B+ 树的磁盘读写代价更低
B+ 树的内部节点并没有指向关键字具体信息的指针。因此其内部节点相对 B 树更小，如果把所有同一内部节点的关键字存放在同一磁盘中，那么磁盘容纳的关键字数量也会增多。一次性读入内存中的查找关键字也会变多，相对来说 IO 读写次数也就降低了。 
2. B+ 树的查询效率更加稳定
由于非终结点并不是最终指向文件内容的节点，而只是叶子节点关键字的索引，所以任何关键字的查找必须走一条从根节点到叶子节点的路，所有关键字查询的路径长度相同，导致每一个数据的查询效率相等。

# 聚集索引

- 聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。
- 优点 
  - 聚集索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。

- 缺点 
  - 依赖于有序的数据 ：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。
  - 更新代价大 ： 如果对索引列的数据修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。

# 非聚集索引

- 非聚集索引即索引结构和数据分开存放的索引。二级索引属于非聚集索引 
- 优点 
  - 更新代价比聚集索引要小 ，因为非聚集索引的叶子节点是不存放数据的

- 缺点 
  - 跟聚集索引一样，非聚集索引也依赖于有序的数据
  - 可能会二次查询（回表） ：这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。

# 覆盖索引

覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 无需回表查询。

# MySQL 中常用的索引有哪些

- 单值索引：即一个索引只包含单个列，一个表可以有多个单列索引
- 唯一索引：索引列的值必须唯一，但允许有空值
- 主键索引：设定为主键后数据库会自动建立索引
- 复合索引：即一个索引包含多个列

# 实际项目中，你是如何建立索引的呢？

首先我会根据业务进行判断，是否需要建立索引

- 如果是中到大型的表，我会考虑建索引，如果是特大型的表，我不会建立索引，因为那样维护起来开销太大了
- 在使用 limit offset 查询缓慢时，我会选择建立索引
- 建表的时候，我会选择与业务无关的自增主键作为主键，而不是采用业务主键，因为主键索引需要依赖有序的数据，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢

建立索引的话我会从以下几个方面入手：

如果非要建索引的话，我一般从以下几个方面入手：

1. 不为 NULL 的字段 ：索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又免不了为 NULL，建议使用 0、1、true、false 这样语义较为清晰的短值或短字符作为替代。
2. 被频繁查询的字段 ：我们创建索引的字段应该是查询操作非常频繁的字段。
3. 被作为条件查询的字段 ：被作为 WHERE 条件查询的字段，应该被考虑建立索引。
4. 频繁需要排序的字段 ：索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。
5. 频繁用于连接的字段 ：频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。
6. 单张表索引不超过 5 个

# 什么情况下设置了索引但无法使用

1. 以“%”开头的 LIKE 语句，模糊匹配
2. OR 语句前后没有同时使用索引
3. 数据类型出现隐式转化（如 varchar 不加单引号的话可能会自动转换为 int 型）

# 大表如何优化

限定数据范围

禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；

读/写分离

经典的数据库拆分方案，主库负责写，从库负责读；

垂直分区

根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。

水平分区

水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。

需要注意的⼀点是：分表仅仅是解决了单⼀表数据过大的问题，但由于表的数据还是在同⼀台机器上，其实对于提升 MySQL 并发能力没有什么意义，所以水平拆分最好分库 。

# MySQL 主从复制原理的是啥？

主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。

有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。

还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。

# 如何解决 MySQL 主从复制的数据丢失问题呢？

打开 MySQL 支持的半同步复制，也叫 **semi-sync** 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。

# 如何解决 MySQL 主从同步的延时问题？

以前遇见过一次，有个同事插入数据之后，又查了一遍，然后又更新了，然后在高峰期从库复制主库的时候，总是有些数据过了高峰期没更新，后来查询从机发现，从库复制主库的数据落后了几 ms。

- 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。
- 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。
- 重写查询逻辑的代码

# 数据库优化

1. 选取最适用的字段属性，尽可能减少定义字段宽度，尽量把字段设置 NOTNULL，例如’省份’、’性别’最好适用 ENUM
2. 使用连接(JOIN)来代替子查询
3. 适用联合(UNION)来代替手动创建的临时表
4. 事务处理
5. 锁定表、优化事务处理
6. 适用外键，优化锁定表
7. 建立索引
8. 优化查询语句

# 单表优化

一般以整型值为主的表在千万级以下，字符串为主的表在五百万以下，是没有太大问题的

索引

- 索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描
- 应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描
- 值分布很稀少的字段不适合建索引，例如"性别"这种只有两三个值的字段
- 字符字段只建前缀索引
- 字符字段最好不要做主键
- 不用外键，由程序保证约束
- 尽量不用UNIQUE，由程序保证约束
- 使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引

查询SQL

- 可通过开启慢查询日志来找出较慢的SQL
- 不做列运算：SELECT id WHERE age + 1 = 10，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边
- sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库
- 不用SELECT *
- OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内
- 不用函数和触发器，在应用程序实现
- 避免%xxx式查询
- 少用JOIN
- 使用同类型进行比较，比如用'123'和'123'比，123和123比
- 尽量避免在WHERE子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描
- 对于连续数值，使用BETWEEN不用IN：SELECT id FROM t WHERE num BETWEEN 1 AND 5
- 列表数据不要拿全表，要使用LIMIT来分页，每页数量也不要太大

# MySQL 数据库作为发布系统的存储，一天五万条以上的增量，预计运维三年,怎么优化？

1. 设计良好的数据库结构，允许部分数据冗余，尽量避免 join 查询，提高效率。
2. 选择合适的表字段数据类型和存储引擎，适当的添加索引。
3. MySQL 库主从读写分离。
4. 找规律分表，减少单表中的数据量提高查询速度。
5. 添加缓存机制，比如 memcached，apc 等。
6. 不经常改动的页面，生成静态页面。
7. 书写高效率的 SQL。比如 SELECT * FROM TABEL 改为 SELECT field_1,field_2, field_3 FROM TABLE.

# 分库分表之后，id 主键如何处理

有以下几种方式

UUID

- 不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。

数据库自增 id

- 两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。

利用 redis 生成 id :

- 性能比较好，灵活方便，不依赖于数据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。

Twitter 的雪花算法

- 它能够保证不同表的主键的不重复性，以及相同表的主键的有序性，MyBatisPlus 的 @TableId 默认采用，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞，并且效率较高。

# SQL 注入漏洞产生的原因？如何防止？

SQL 注入产生的原因

- 程序开发过程中不注意规范书写 sql 语句和对特殊字符进行过滤，导致客户端可以通过全局变量 POST 和 GET 提交一些 sql 语句正常执行。

防止 SQL 注入的方式：

- 开启配置文件中的 magic_quotes_gpc 和 magic_quotes_runtime 设置 执行 sql 语句时使用 addslashes 进行 sql 语句转换
- SQL 语句书写尽量不要省略双引号和单引号。
- 过滤掉 SQL 语句中的一些关键词：update、insert、delete、select、 * 。
- 提高数据库表和字段的命名技巧，对一些重要的字段根据程序的特点命名，取不易被猜到的。

# 为什么要分库分表

MySQL 单机部署，扛不住高并发、 单机磁盘容量几乎撑满、单表数据量太大，SQL 越跑越慢，就是你一个库来说，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。

对于表来说的话，一般几百万数据，性能就会相对差一些，此时就需要分表了，分表就是把一个表的数据放到多个表中，然后查询的时候就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表的数据量就固定在 200 万以内。

分库分表之后，MySQL 从单机到多机，能承受的并发增加了多倍；拆分为多个库，数据库服务器磁盘使用率大大降低；单表数据量减少，SQL 执行效率明显提升；

# 用过哪些分库分表中间件？

Sharding-jdbc

当当开源的，属于 client 层方案，是 `ShardingSphere` 的 client 层方案，ShardingSphere还提供 proxy 层的方案 Sharding-Proxy。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。

Mycat

基于 Cobar 改造的，属于 proxy 层方案，支持的功能非常完善

# 不同的分库分表中间件都有什么优点和缺点？

Sharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 Sharding-jdbc 的依赖；

Mycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。

我们用的 Sharding-jdbc，client 层方案轻便，而且维护成本低，再说系统也没有那么复杂，项目也没那么多

# 你们具体是如何将那个单库单表的系统给迁移到分库分表

我们主要采用的是垂直拆分，我们基础的商品、库存、优惠、用户、优惠券服务，都是对应实际微服务拆分的，并且针对数据库里面的具体的一些表，比如商品系统，我们针对商品的spuId、skuId，就分了 3 到 4 张表，还有订单系统，根据 orderId 我们也分为了好几张表，有订单详情、订单项。

# 怎么将单库单表的系统给迁移到分库分表上去？

我们最开始是停机维护

0 点到 6 点，系统不能访问，接着到 0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了。然后用之前写好的一个导数的一次性工具，然后将单库单表的数据读出来，写到分库分表里面去，导数完了之后，就 ok 了，修改系统的数据库连接配置啥的，包括可能代码和 SQL 也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。

后来我们采用双写迁移方案

就是在线上系统里面，之前所有写库的地方，增删改操作，除了对老库增删改，再加上对新库的增删改，这就是所谓的双写，同时写两个库，老库和新库。

然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。简单来说，就是不允许用老数据覆盖新数据。

导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。

接着当数据完全一致了，就 ok 了，基于仅仅使用分库分表的最新代码，重新部署一次。

# 如何动态扩容缩容分库分表方案

直接分出 32 个库，每个库 32 个表，那么总共是 1024 张表。

每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载 32  1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32  1500 = 48000 的写并发，接近 5 万每秒的写入并发，前面再加一个 MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。

1024 张表，假设每个表放 500 万数据，在 MySQL 里可以放 50 亿条数据。

每秒 5 万的写并发，总共 50 亿条数据，一般够中小型公司用好几年了

具体操作是由 DBA 完成的，以后要减少库的数量，也很简单，就是按倍数缩容就可以了，然后修改一下路由规则。